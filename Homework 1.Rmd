---
title: "Homework 1"
author: "Brett Adlard"
date: "1/12/2019"
output: html_document
---

Linear and Non-Linear Models (31010)
Week 1: Homework Assignment
Â© Yuri Balasanov iLykei.com

1 Assignment description and preparation for test
1.1 Reading data
1.2 Reproducing outputs of glm() using lm()
2 Test
This project helps understanding the linear model as a particular case of generalized linear model
This project is individual

1 Assignment description and preparation for test
In preparation for the test below calculate different outputs of glm() using lm().

1.1 Reading data
Use the data from file Week1_Homework_Project_Data.csv to estimate linear model with lm().
Analize summary() of the estimated linear model.
```{r}
dataPath <- "/Users/brettadlard/OneDrive/UChicago/Linear and Non-Linear/"
Linear.Model.Data<-read.csv(file=paste(dataPath,"Week1_Homework_Project_Data.csv",sep="/"),header=TRUE,sep=",")
head(Linear.Model.Data,10)
```

```{r}
LM <- lm(Output~Input1 + Input2 + Input3,data=Linear.Model.Data)
GLM <- glm(Output~Input1 + Input2 + Input3,family=gaussian(link="identity"),data=Linear.Model.Data)
```



##       Output   Input1   Input2       Input3
## 1   5.451630 1.875976 2.721764 -0.011754874
## 2   4.544792 1.713108 2.570596  0.007136568
## 3   5.345051 2.804193 4.700366  0.056975906
## 4  11.239833 3.895376 6.863106 -0.103290465
## 5  10.055252 3.694171 6.405116 -0.142105913
## 6   1.720138 1.104742 1.203397  0.241145841
## 7   9.010973 2.947958 4.968758 -0.059639593
## 8   5.860532 2.743866 4.545334  0.048181014
## 9  13.368089 4.595102 8.226541 -0.189332876
## 10 10.193232 3.639261 6.228819 -0.074765940
Fit linear model.

What can you tell about the data and the fit?

1.2 Reproducing outputs of glm() using lm()
By using any outputs of lm() or summary(lm.fit) calculate the following outputs of glm().

You can use glm() to check your answers, but, please, do not use glm object or any functions applied to glm object to calculate your results.

Calculate variables:

```{r}
cbind(Output.LM=LM$coefficients,Output.GLM=GLM$coefficients)
```

coefficients (5%)
residuals (5%)
```{r}
cbind(Output.LM=LM$residuals,Output.GLM=GLM$residuals)[1:10,]
```

fitted.values (5%)
```{r}
(LM$fitted.values==GLM$fitted.values)[1:10]
sum(abs(Linear.Model.Data.lm$fitted.values-Linear.Model.Data.glm$fitted.values))
```

linear.predictors (10%)
```{r}
sum(abs(Linear.Model.Data.lm$fitted.values-Linear.Model.Data.glm$linear.predictors))
```

deviance (25%)
```{r}
c(From.GLM=GLM$deviance,
  From.LM=sum(LM$residuals^2),
  Function.Deviance=deviance(LM))
```

Use deviance() (10%)

Calculate deviance manually based on the definition given in the lecture (15%)
```{r}
manual.deviance <- sum(LM$residuals^2)
manual.deviance
```

```{r}
Manual.Log.Likelihood<- function(Linear.Model.Fit) {
  my.Total.Length<-length(Linear.Model.Fit$residuals)
  my.Number.Of.Parameters<-Linear.Model.Fit$rank
  my.Variance.Estimate<-summary(Linear.Model.Fit)$sigma^2*(my.Total.Length-my.Number.Of.Parameters)/my.Total.Length
  (-my.Total.Length/2*log(2*pi*my.Variance.Estimate)-sum(Linear.Model.Fit$residuals^2)/2/my.Variance.Estimate)
}
Log.Likelihood<-Manual.Log.Likelihood(LM)
AIC.Manual<-(-2*(Log.Likelihood)+2*(LM$rank+1))
c(AIC.Manual=AIC.Manual,AIC.From.Function=From.AIC.Function,AIC.From.GLM=AIC.From.glm)
```

Akaike Information Criterion aic (25%)
```{r}
From.AIC.Function<-AIC(LM)
AIC.From.glm<-GLM$aic
c(From.AIC.Function,AIC.From.glm)
```

Obtain it by using AIC() (10%)
```{r}
AIC(LM)
```

Calculate it manually using the definition given in the lecture. (15%)
```{r}
Manual.Log.Likelihood<- function(Linear.Model.Fit) {
  my.Total.Length<-length(Linear.Model.Fit$residuals)
  my.Number.Of.Parameters<-Linear.Model.Fit$rank
  my.Variance.Estimate<-summary(Linear.Model.Fit)$sigma^2*(my.Total.Length-my.Number.Of.Parameters)/my.Total.Length
  (-my.Total.Length/2*log(2*pi*my.Variance.Estimate)-sum(Linear.Model.Fit$residuals^2)/2/my.Variance.Estimate)
}
Log.Likelihood<-Manual.Log.Likelihood(LM)
AIC.Manual<-(-2*(Log.Likelihood)+2*(LM$rank+1))
AIC.Manual
```

y (5%)
```{r}
sum(abs(Linear.Model.Data[,1]-GLM$y))
```

null.deviance (10%)
```{r}
Linear.Model.Data.Null.lm<-lm(Output~1,data=Linear.Model.Data)
Linear.Model.Data.Null.lm.SSE<-sum(Linear.Model.Data.Null.lm$residuals^2)
c(Null.SSE.lm=Linear.Model.Data.Null.lm.SSE,Null.Deviance.glm=GLM$null.deviance)
```

dispersion (10%)
```{r}
summary(GLM)$dispersion
```
## [1] 1.002509
```{r}
summary(LM)$sigma^2
```
## [1] 1.002509
```{r}
var(LM$residuals)*(length(Linear.Model.Data[,1])-1)/(length(Linear.Model.Data[,1])-2)
```
## [1] 1.002509
```{r}
var(GLM$residuals)*(length(Linear.Model.Data[,1])-1)/
  (length(Linear.Model.Data[,1])-GLM$rank)
```
As an example, here is solution for the second question.

Residuals.
In the case when data satisfy assumptions of linear moder and identity is selected as link, residuals returned by glm() and lm() are the same.
```{r}
matplot(1:length(Linear.Model.Data[,1]),cbind(Linear.Model.Data.lm$residuals,Linear.Model.Data.glm$residuals),type="l",ylab="Residuals",xlab="Count")
```



```{r}
sum(abs(Linear.Model.Data.lm$residuals-Linear.Model.Data.glm$residuals)>.00000000001)
```


## [1] 0
The last output shows that maximum deviation of Linear.Model.Data.lm$residuals from Returned.from.glm$residuals in absolute value is less than or equal to 0.00000000001.

To answer the question create vector of residuals
```{r}
res<-Linear.Model.Data.lm$residuals
```


Continue answering the questions of this section.